{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем модель для детекции уклончивости с huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link for english model](https://huggingface.co/alenaa/evasiveness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alena.komarova/opt/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alenaa/evasiveness\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"alenaa/evasiveness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"local-pt-checkpoint\", safe_serialization=False)  \n",
    "model.save_pretrained(\"local-pt-checkpoint\", safe_serialization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Конвертируем в формат onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install 'transformers[onnx]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install onnxruntime -c conda-forge -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local PyTorch model found.\n",
      "Framework not requested. Using torch to export to ONNX.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', '-m', 'transformers.onnx', '--model=local-pt-checkpoint', '--feature=sequence-classification', 'onnx/'], returncode=-11)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess  \n",
    "\n",
    "subprocess.run(f\"python -m transformers.onnx --model=local-pt-checkpoint --feature=sequence-classification onnx/\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime \n",
    "\n",
    "onnx_session = onnxruntime.InferenceSession(\n",
    "    'onnx/model.onnx',\n",
    "    providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестируем модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказываем с помощью ONNX модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import time \n",
    "import tracemalloc\n",
    "\n",
    "def predict_onnx(feed):\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "    output = onnx_session.run(None, feed)\n",
    "    mem = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    end_time = time.time() - start_time\n",
    "    output_tensor = torch.tensor(output[0], dtype=torch.float32)\n",
    "    softmax_output = torch.nn.functional.softmax(output_tensor, dim=-1)\n",
    "    predictions = np.squeeze(softmax_output.numpy())\n",
    "    return(np.argmax(predictions, axis=1)), end_time, mem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказываем с помощью обычной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tracemalloc.start()\n",
    "        start_time = time.time()\n",
    "        outputs = model(**inputs)\n",
    "        end_time = time.time() - start_time\n",
    "        mem = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        return predictions, end_time, mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим точность и время на сгенерированном датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('english_data_200.csv', index_col = 0)\n",
    "Q = data.Question.values.tolist()\n",
    "A = data.Answer.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"local-pt-checkpoint\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"local-pt-checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(Q, A, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "feed = dict(\n",
    "        input_ids=np.array(inputs[\"input_ids\"]).astype(\"int64\"),\n",
    "        attention_mask=np.array(inputs[\"attention_mask\"]).astype(\"int64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test = data.Label.values.tolist()\n",
    "\n",
    "def print_results(prediction_func, inp):\n",
    "    test = data.Label.values.tolist()\n",
    "    preds, time_, mem = prediction_func(inp)\n",
    "    mse = accuracy_score(test, preds.tolist())\n",
    "    if prediction_func == predict:\n",
    "        model_type = 'Simple Model'\n",
    "    else:\n",
    "        model_type = 'ONNX Model'\n",
    "    print(f'{model_type}')\n",
    "    print('MSE: ', mse)\n",
    "    print('Time: ', time_)\n",
    "    print('Memory: ', mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Model\n",
      "MSE:  0.7548076923076923\n",
      "Time:  3.149432897567749\n",
      "Memory:  (5114, 9582)\n"
     ]
    }
   ],
   "source": [
    "print_results(predict, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model\n",
      "MSE:  0.7548076923076923\n",
      "Time:  2.1993188858032227\n",
      "Memory:  (3129, 5582)\n"
     ]
    }
   ],
   "source": [
    "print_results(predict_onnx, feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем оптимизировать модель onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnxruntime_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: onnxruntime_tools is deprecated. Use onnxruntime or onnxruntime-gpu instead. For more information, see https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/README.md.\n",
      "\n",
      "ONNX Model\n",
      "MSE:  0.7548076923076923\n",
      "Time:  1.9741268157958984\n",
      "Memory:  (1992, 2143)\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime_tools import optimizer\n",
    "\n",
    "optimized_model = optimizer.optimize_model(\"onnx/model.onnx\", model_type='bert')\n",
    "optimized_model.save_model_to_file(\"onnx/opt_model.onnx\")\n",
    "onnx_session = onnxruntime.InferenceSession(\n",
    "    'onnx/opt_model.onnx',\n",
    "    providers=['CPUExecutionProvider'])\n",
    "print_results(predict_onnx, feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_name",
   "language": "python",
   "name": "env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
