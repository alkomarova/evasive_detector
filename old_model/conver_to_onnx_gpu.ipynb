{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем модель для детекции уклончивости с huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link for english model](https://huggingface.co/alenaa/evasiveness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alenaa/evasiveness\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"alenaa/evasiveness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"local-pt-checkpoint\", safe_serialization=False)  \n",
    "model.save_pretrained(\"local-pt-checkpoint\", safe_serialization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Конвертируем в формат onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install 'transformers[onnx]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnxruntime_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess  \n",
    "\n",
    "subprocess.run(f\"python -m transformers.onnx --model=local-pt-checkpoint --feature=sequence-classification onnx/\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-05-30 16:03:44.858964400 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-05-30 16:03:44.858987186 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime \n",
    "\n",
    "onnx_session = onnxruntime.InferenceSession(\n",
    "    'onnx/model.onnx',\n",
    "    providers=['CUDAExecutionProvider'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестируем модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказываем с помощью ONNX модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import time \n",
    "import tracemalloc\n",
    "\n",
    "def predict_onnx(feed):\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "    output = onnx_session.run(None, feed)\n",
    "    mem = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    end_time = time.time() - start_time\n",
    "    output_tensor = torch.tensor(output[0], dtype=torch.float32)\n",
    "    softmax_output = torch.nn.functional.softmax(output_tensor, dim=-1)\n",
    "    predictions = np.squeeze(softmax_output.numpy())\n",
    "    return(np.argmax(predictions, axis=1)), end_time, mem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказываем с помощью обычной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tracemalloc.start()\n",
    "        start_time = time.time()\n",
    "        outputs = model(**inputs)\n",
    "        end_time = time.time() - start_time\n",
    "        mem = tracemalloc.get_traced_memory()\n",
    "        tracemalloc.stop()\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        return predictions, end_time, mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим точность и время на сгенерированном датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('english_data_200.csv', index_col = 0)\n",
    "Q = data.Question.values.tolist()\n",
    "A = data.Answer.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1383: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"local-pt-checkpoint\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"local-pt-checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(Q, A, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "feed = dict(\n",
    "        input_ids=np.array(inputs[\"input_ids\"]).astype(\"int64\"),\n",
    "        attention_mask=np.array(inputs[\"attention_mask\"]).astype(\"int64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test = data.Label.values.tolist()\n",
    "\n",
    "def print_results(prediction_func, inp):\n",
    "    test = data.Label.values.tolist()\n",
    "    preds, time_, mem = prediction_func(inp)\n",
    "    mse = accuracy_score(test, preds.tolist())\n",
    "    if prediction_func == predict:\n",
    "        model_type = 'Simple Model'\n",
    "    else:\n",
    "        model_type = 'ONNX Model'\n",
    "    print(f'{model_type}')\n",
    "    print('MSE: ', mse)\n",
    "    print('Time: ', time_)\n",
    "    print('Memory: ', mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Model\n",
      "MSE:  0.7548076923076923\n",
      "Time:  0.928778886795044\n",
      "Memory:  (679097, 706469)\n"
     ]
    }
   ],
   "source": [
    "print_results(predict, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model\n",
      "MSE:  0.7548076923076923\n",
      "Time:  1.4862103462219238\n",
      "Memory:  (4698, 7312)\n"
     ]
    }
   ],
   "source": [
    "print_results(predict_onnx, feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем оптимизировать модель onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnxruntime_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: onnxruntime_tools is deprecated. Use onnxruntime or onnxruntime-gpu instead. For more information, see https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/README.md.\n",
      "\n",
      "ONNX Model\n",
      "MSE:  0.7548076923076923\n",
      "Time:  0.07584977149963379\n",
      "Memory:  (1993, 2080)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2024-05-30 16:04:24.539675713 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2024-05-30 16:04:24.539696471 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime_tools import optimizer\n",
    "\n",
    "optimized_model = optimizer.optimize_model(\"onnx/model.onnx\", model_type='bert')\n",
    "optimized_model.save_model_to_file(\"onnx/opt_model.onnx\")\n",
    "onnx_session = onnxruntime.InferenceSession(\n",
    "    'onnx/opt_model.onnx',\n",
    "    providers=['CUDAExecutionProvider'])\n",
    "print_results(predict_onnx, feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
